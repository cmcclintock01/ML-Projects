{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cdb32290-04af-4b22-99e5-728a9e68d797",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 AB    H   SO   BB  HR    AVG    OBP    SLG    OPS  AVG_group\n",
      "Player                                                                       \n",
      "AJ Pollock      489  120   98   32  14  0.245  0.292  0.389  0.681          5\n",
      "Aaron Hicks     384   83  109   62   8  0.216  0.330  0.313  0.643          4\n",
      "Aaron Judge     570  177  175  111  62  0.311  0.425  0.686  1.111          8\n",
      "Abraham Toro    324   60   65   22  10  0.185  0.239  0.324  0.563          3\n",
      "Adam Duvall     287   61  101   21  12  0.213  0.276  0.401  0.677          4\n",
      "...             ...  ...  ...  ...  ..    ...    ...    ...    ...        ...\n",
      "Yonathan Daza   372  112   58   26   2  0.301  0.349  0.384  0.733          8\n",
      "Yordan Alvarez  470  144  106   78  37  0.306  0.406  0.613  1.019          8\n",
      "Yoshi Tsutsugo  170   29   50   19   2  0.171  0.249  0.229  0.478          2\n",
      "Yuli Gurriel    545  132   73   30   8  0.242  0.288  0.360  0.648          5\n",
      "Zach McKinstry  155   32   48   13   4  0.206  0.272  0.361  0.633          4\n",
      "\n",
      "[450 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_2022 = pd.read_csv(\"2022stats.csv\")\n",
    "\n",
    "##First we will drop the columns that we do not need.\n",
    "copy_2022 = data_2022.copy()\n",
    "copy_2022.sort_values(\"Player\")\n",
    "copy_2022.drop(labels = ['G', 'Team', 'R', '2B', '3B', 'RBI', 'SB', 'CS', 'SH', 'SF', 'HBP'],axis='columns', inplace=True) \n",
    "\n",
    "#Now I will delete people that have lower than 100 At Bats(AB)\n",
    "above_100_2022 = copy_2022['AB'] >= 100\n",
    "\n",
    "copy_2022 = copy_2022[above_100_2022]\n",
    "\n",
    "\n",
    "#Now we will combine the names that have multiple instances\n",
    "copy_2022 = copy_2022.groupby('Player').agg({'AB' : 'sum', 'H' : 'sum','SO' : 'sum',  'BB' : 'sum', 'HR' : 'sum', 'AVG' : 'mean', 'OBP' : 'mean', 'SLG' : 'mean', 'OPS' : 'mean'})\n",
    "def avg_group(x):\n",
    "    if x < .150: return 1\n",
    "    if x >= .150 and x < .175: return 2\n",
    "    if x >= .175 and x < .200: return 3\n",
    "    if x >= .200 and x < .225: return 4\n",
    "    if x >= .225 and x < .250: return 5\n",
    "    if x >= .250 and x < .275: return 6\n",
    "    if x >= .275 and x < .300: return 7\n",
    "    if x >= .300 and x < .325: return 8\n",
    "    if x >= .325 and x < .350: return 9\n",
    "    if x >= .350: return 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "copy_2022['AVG_group'] = copy_2022['AVG'].apply(avg_group)\n",
    "\n",
    "print(copy_2022)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_2022, test_2022 = train_test_split(copy_2022, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd59707-aa3f-4e5d-9ca6-f05aea354cf5",
   "metadata": {},
   "source": [
    "# Markdown 1\n",
    "I plan on using the average from 2022 and the groups I placed them in for this decision tree. I believe this one should have near perfect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "920074aa-3ce2-4909-b471-50556236bd5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  0  0  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0]\n",
      " [ 0  0 22  0  0  0  0  0]\n",
      " [ 0  0  0 28  0  0  0  0]\n",
      " [ 0  0  0  0 17  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  3  0]\n",
      " [ 0  0  0  0  0  0  1  0]]\n",
      "Accuracy is  0.9888888888888889\n",
      "Precision is  0.9805555555555555\n",
      "Sensitivity is  0.9888888888888889\n",
      "F1 is  0.9841269841269841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s544086\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "X = train_2022[['AVG']]\n",
    "Y = train_2022['AVG_group']\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(X,Y)\n",
    "\n",
    "X = test_2022[['AVG']]\n",
    "Y = test_2022['AVG_group']\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = tree_classifier.predict(X)\n",
    "matrix = confusion_matrix(Y, y_pred)\n",
    "print(matrix)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(\"Accuracy is \", accuracy_score(Y, y_pred))\n",
    "print(\"Precision is \", precision_score(Y, y_pred, average='weighted'))\n",
    "print(\"Sensitivity is \", recall_score(Y, y_pred, average='weighted'))\n",
    "print(\"F1 is \", f1_score(Y, y_pred, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b4c3c-43e4-443a-a405-1f6d4bd2bee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Markdown 2\n",
    "The first analysis did very well it looks like based on the average it is a reasonable that you would be able to place the player in the correct category. Now I am going to take a look at the strike outs compared to average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fe368fac-f43c-4e4b-bb8a-d1c2f69bce33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  2  1  0  0  0]\n",
      " [ 0  1  1  5  2  2  0  0  0]\n",
      " [ 1  0  2  7  6  6  0  0  0]\n",
      " [ 1  2  0  7 12  6  0  0  0]\n",
      " [ 1  1  0  4  3  4  4  0  0]\n",
      " [ 0  0  0  2  1  2  0  0  0]\n",
      " [ 0  0  0  0  2  1  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0]]\n",
      "Accuracy is  0.26666666666666666\n",
      "Precision is  0.2722642517125276\n",
      "Sensitivity is  0.26666666666666666\n",
      "F1 is  0.26001414702422543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s544086\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\s544086\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "X = train_2022[['SO']]\n",
    "Y = train_2022['AVG_group']\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(X,Y)\n",
    "\n",
    "X = test_2022[['SO']]\n",
    "Y = test_2022['AVG_group']\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = tree_classifier.predict(X)\n",
    "matrix = confusion_matrix(Y, y_pred)\n",
    "print(matrix)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(\"Accuracy is \", accuracy_score(Y, y_pred))\n",
    "print(\"Precision is \", precision_score(Y, y_pred, average='weighted'))\n",
    "print(\"Sensitivity is \", recall_score(Y, y_pred, average='weighted'))\n",
    "print(\"F1 is \", f1_score(Y, y_pred, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23632e-1c3e-41b5-a3fd-4f839a14e7c4",
   "metadata": {},
   "source": [
    "This did not do well. There was only a 26% accuracy. It makes sense because the correlation between SO and AVG was very low. Next we will look at Hits vs Average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "41903d91-7818-4eb8-a0f3-8cd8cb5337ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  2  0  1  0  0]\n",
      " [ 2  4  2  3  0  0  0  0]\n",
      " [ 2  1  7  7  3  1  1  0]\n",
      " [ 3  2  6 11  4  2  0  0]\n",
      " [ 0  0  5  3  6  1  2  0]\n",
      " [ 0  0  0  1  3  1  0  0]\n",
      " [ 0  0  1  1  0  1  0  0]\n",
      " [ 0  0  0  0  0  1  0  0]]\n",
      "Accuracy is  0.32222222222222224\n",
      "Precision is  0.3513227513227513\n",
      "Sensitivity is  0.32222222222222224\n",
      "F1 is  0.33336365041791394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s544086\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = train_2022[['H']]\n",
    "Y = train_2022['AVG_group']\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(X,Y)\n",
    "\n",
    "X = test_2022[['H']]\n",
    "Y = test_2022['AVG_group']\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = tree_classifier.predict(X)\n",
    "matrix = confusion_matrix(Y, y_pred)\n",
    "print(matrix)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(\"Accuracy is \", accuracy_score(Y, y_pred))\n",
    "print(\"Precision is \", precision_score(Y, y_pred, average='weighted'))\n",
    "print(\"Sensitivity is \", recall_score(Y, y_pred, average='weighted'))\n",
    "print(\"F1 is \", f1_score(Y, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe2d8b-e92b-47d6-8c4b-53eebdbe4c09",
   "metadata": {},
   "source": [
    "The hits were able to do better than the SO but I feel like there should still be a stronger correlation between average and the other feature. I am going to test more features and find which one does the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "28dbc98a-43c4-41a2-a792-5db6b6b6062c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  2  0  0  0  0  0]\n",
      " [ 0  2  6  2  0  1  0  0]\n",
      " [ 0  4 10  3  5  0  0  0]\n",
      " [ 0  3 10  8  7  0  0  0]\n",
      " [ 0  0  3 10  4  0  0  0]\n",
      " [ 0  0  2  0  1  2  0  0]\n",
      " [ 0  0  0  1  1  0  1  0]\n",
      " [ 0  0  0  0  0  0  1  0]]\n",
      "Accuracy is  0.3111111111111111\n",
      "Precision is  0.3339506172839506\n",
      "Sensitivity is  0.3111111111111111\n",
      "F1 is  0.31001221001220997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s544086\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = train_2022[['OBP']]\n",
    "Y = train_2022['AVG_group']\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(X,Y)\n",
    "\n",
    "X = test_2022[['OBP']]\n",
    "Y = test_2022['AVG_group']\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = tree_classifier.predict(X)\n",
    "matrix = confusion_matrix(Y, y_pred)\n",
    "print(matrix)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(\"Accuracy is \", accuracy_score(Y, y_pred))\n",
    "print(\"Precision is \", precision_score(Y, y_pred, average='weighted'))\n",
    "print(\"Sensitivity is \", recall_score(Y, y_pred, average='weighted'))\n",
    "print(\"F1 is \", f1_score(Y, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f0202aa1-85d6-41ac-a7e9-79de4a7de67f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  1  0  0  0  0  0]\n",
      " [ 2  2  3  2  1  1  0  0  0]\n",
      " [ 0  1  4  3  9  3  2  0  0]\n",
      " [ 2  3  2  6 10  3  2  0  0]\n",
      " [ 0  0  0  3  7  6  0  0  1]\n",
      " [ 0  0  0  1  2  2  0  0  0]\n",
      " [ 0  0  0  2  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0]]\n",
      "Accuracy is  0.24444444444444444\n",
      "Precision is  0.2533333333333333\n",
      "Sensitivity is  0.24444444444444444\n",
      "F1 is  0.24811302681992337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s544086\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\s544086\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = train_2022[['SLG']]\n",
    "Y = train_2022['AVG_group']\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(X,Y)\n",
    "\n",
    "X = test_2022[['SLG']]\n",
    "Y = test_2022['AVG_group']\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = tree_classifier.predict(X)\n",
    "matrix = confusion_matrix(Y, y_pred)\n",
    "print(matrix)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(\"Accuracy is \", accuracy_score(Y, y_pred))\n",
    "print(\"Precision is \", precision_score(Y, y_pred, average='weighted'))\n",
    "print(\"Sensitivity is \", recall_score(Y, y_pred, average='weighted'))\n",
    "print(\"F1 is \", f1_score(Y, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4727bc77-5b86-4b94-a69e-3c5b3194e1db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  1  0  1  1  0  0]\n",
      " [ 0  0  2  6  3  0  0  0]\n",
      " [ 0  0  8  9  5  0  0  0]\n",
      " [ 0  0  7 14  7  0  0  0]\n",
      " [ 0  0  9  3  4  1  0  0]\n",
      " [ 0  0  1  2  2  0  0  0]\n",
      " [ 0  0  1  1  0  1  0  0]\n",
      " [ 0  0  1  0  0  0  0  0]]\n",
      "Accuracy is  0.28888888888888886\n",
      "Precision is  0.22397306397306396\n",
      "Sensitivity is  0.28888888888888886\n",
      "F1 is  0.2522317188983855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s544086\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = train_2022[['BB']]\n",
    "Y = train_2022['AVG_group']\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(X,Y)\n",
    "\n",
    "X = test_2022[['BB']]\n",
    "Y = test_2022['AVG_group']\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = tree_classifier.predict(X)\n",
    "matrix = confusion_matrix(Y, y_pred)\n",
    "print(matrix)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(\"Accuracy is \", accuracy_score(Y, y_pred))\n",
    "print(\"Precision is \", precision_score(Y, y_pred, average='weighted'))\n",
    "print(\"Sensitivity is \", recall_score(Y, y_pred, average='weighted'))\n",
    "print(\"F1 is \", f1_score(Y, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2af291-e45c-427c-9a2f-f74b180a91be",
   "metadata": {},
   "source": [
    "# The Test Set\n",
    "In each decision tree classifier I trained my data using the training set and then tested it on the test set. If I were to use the training set in finding my metrics it would be overfit because the decision tree would know the what the data that I'm trying to guess is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bd3eb-6351-49c2-8cd0-b703488b1f3f",
   "metadata": {},
   "source": [
    "# Try out SVC\n",
    "\n",
    "I want to use SVC and see if it is able to get the same results that my DecisionTreeClassifier was able to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "05189814-bb20-4a05-a942-b8697f799d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  0  0  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0]\n",
      " [ 0  0 22  0  0  0  0  0]\n",
      " [ 0  0  0 28  0  0  0  0]\n",
      " [ 0  0  0  0 17  0  0  0]\n",
      " [ 0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  3  0]\n",
      " [ 0  0  0  0  0  0  1  0]]\n",
      "Accuracy is  0.9888888888888889\n",
      "Precision is  0.9805555555555555\n",
      "Sensitivity is  0.9888888888888889\n",
      "F1 is  0.9841269841269841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s544086\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "X = train_2022[['AVG']]\n",
    "Y = train_2022['AVG_group']\n",
    "\n",
    "svm_classifier = SVC(kernel='poly')\n",
    "svm_classifier.fit(X, Y)\n",
    "\n",
    "X = test_2022[['AVG']]\n",
    "Y = test_2022['AVG_group']\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_predicted = svm_classifier.predict(X)\n",
    "matrix = confusion_matrix(Y, y_predicted)\n",
    "print(matrix)\n",
    "print (\"Accuracy is \", accuracy_score(Y, y_predicted))\n",
    "print (\"Precision is \", precision_score(Y, y_predicted, average=\"weighted\"))\n",
    "print (\"Sensitivity is \", recall_score(Y, y_predicted, average=\"weighted\"))\n",
    "print (\"F1 is \", f1_score(Y, y_predicted, average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d8589e74-5889-49a5-85ac-20d124b79ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  3  0  0  0  0  0]\n",
      " [ 0  0 10  1  0  0  0  0]\n",
      " [ 0  0 19  3  0  0  0  0]\n",
      " [ 0  0 16  4  8  0  0  0]\n",
      " [ 0  0  4  2 11  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0]\n",
      " [ 0  0  0  1  2  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0]]\n",
      "Accuracy is  0.37777777777777777\n",
      "Precision is  0.2794022849578405\n",
      "Sensitivity is  0.37777777777777777\n",
      "F1 is  0.2837876337876338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s544086\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "X = train_2022[['H']]\n",
    "Y = train_2022['AVG_group']\n",
    "\n",
    "svm_classifier = SVC(kernel='poly')\n",
    "svm_classifier.fit(X, Y)\n",
    "\n",
    "X = test_2022[['H']]\n",
    "Y = test_2022['AVG_group']\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_predicted = svm_classifier.predict(X)\n",
    "matrix = confusion_matrix(Y, y_predicted)\n",
    "print(matrix)\n",
    "print (\"Accuracy is \", accuracy_score(Y, y_predicted))\n",
    "print (\"Precision is \", precision_score(Y, y_predicted, average=\"weighted\"))\n",
    "print (\"Sensitivity is \", recall_score(Y, y_predicted, average=\"weighted\"))\n",
    "print (\"F1 is \", f1_score(Y, y_predicted, average=\"weighted\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "86e374ea-3af9-457d-add0-8dfe1cc5f667",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  3  0  0  0]\n",
      " [ 0  0  0  0 11  0  0  0]\n",
      " [ 0  0  0  0 22  0  0  0]\n",
      " [ 0  0  0  0 28  0  0  0]\n",
      " [ 0  0  0  0 17  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  3  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0]]\n",
      "Accuracy is  0.18888888888888888\n",
      "Precision is  0.035679012345679016\n",
      "Sensitivity is  0.18888888888888888\n",
      "F1 is  0.06002076843198338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s544086\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "X = train_2022[['SO']]\n",
    "Y = train_2022['AVG_group']\n",
    "\n",
    "svm_classifier = SVC(kernel='poly')\n",
    "svm_classifier.fit(X, Y)\n",
    "\n",
    "X = test_2022[['SO']]\n",
    "Y = test_2022['AVG_group']\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_predicted = svm_classifier.predict(X)\n",
    "matrix = confusion_matrix(Y, y_predicted)\n",
    "print(matrix)\n",
    "print (\"Accuracy is \", accuracy_score(Y, y_predicted))\n",
    "print (\"Precision is \", precision_score(Y, y_predicted, average=\"weighted\"))\n",
    "print (\"Sensitivity is \", recall_score(Y, y_predicted, average=\"weighted\"))\n",
    "print (\"F1 is \", f1_score(Y, y_predicted, average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a178abf-346d-44a3-a583-f04eecea3cb4",
   "metadata": {},
   "source": [
    "SVC did much worse that decision trees did. I believe because the decision tree is much better at dealing with categorical data. I changed my data and made it categorical so the decision tree was going to work much better. I tried all of the different kernels in SVC but it still did not help the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92ed19-3296-4135-9697-c667006c7ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
